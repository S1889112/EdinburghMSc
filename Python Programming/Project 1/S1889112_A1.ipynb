{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project tasks\n",
    "\n",
    "- Please rename this file so that you know which copy you have been working in. Keep a copy safe (especially if you are working in the online Jupyter service). You can download a copy by choosing -File- then -Download as- Notebook from the menu above. \n",
    "- Complete all of the tasks \n",
    "- Make sure to use good code style, and comment your code appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Task 1 - Code review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This task is to write a code review, *not* to write python code to solve the problem brief. \n",
    "\n",
    "A colleague has been asked to write some code to answer the following brief:\n",
    "\n",
    "---\n",
    "\n",
    "### Brief:\n",
    "The determinant of an $n\\times n$ matrix $A$ can be calculated recursively by \"row/column expansion\", for any given column $j$:\n",
    "\n",
    "$$\n",
    "\\textrm{det}(A) = \\sum_{i=1}^n (âˆ’1)^{(i+j)}\\,\\,a_{ij}\\,\\, \\textrm{det}(\\bar{A}_{ij})\n",
    "$$\n",
    "\n",
    "where $a_{ij}$ is the element of $A$ in the $i$-th row and $j$-th column, and $\\bar{A}_{ij}$ is the $(n âˆ’ 1)\\times (n âˆ’ 1)$ matrix obtained from $A$ by deleting the $i$-th row and $j$-th column.\n",
    "\n",
    "The above formula works for any $j$ (you can just use $j = 1$: expansion of the first column).\n",
    "\n",
    "You should:\n",
    "\n",
    "* Write a function to (recursively) work out the determinant of a two-dimensional Numpy array without using the inbuilt functions. \n",
    "\n",
    "* Write a further function which determines the volume of the parallelepiped spanned by the vectors $(x_1,y_1,z_1)$, $(x_2,y_2,z_2)$, $(x_3,y_3,z_3)$, which is given by the absolute value of:\n",
    "\n",
    "$$\n",
    "\\textrm{det}\\left(\\begin{array}{ccc}\n",
    "x_1 & y_1 & z_1\\\\\n",
    "x_2 & y_2 & z_2\\\\\n",
    "x_3 & y_3 & z_3\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "using your determinant function.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Your task:\n",
    "\n",
    "You have been asked to write a review of their code. Here is the code they produced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def my_det(A,n):\n",
    "# Calculate the determinant of a matrix\n",
    "    if n==1:\n",
    "        return A[0,0]  #Return a single value in this case\n",
    "    else:\n",
    "     det = 0.0\n",
    "     for i in range(0,n-1):\n",
    "          # Remove the row and column:  \n",
    "          B = delete(A,i,axis=0)\n",
    "          C = delete(B,0,axis=1)\n",
    "          #print(C)\n",
    "          det = det + (-1)**(i+1)*A[i,0]*my_det(C,n-1) #implement the formula\n",
    "    return det\n",
    "\n",
    "def paraVol(a,b,c):\n",
    "  '''\n",
    "  Get the volume of a parallelepiped spanned by a,b,c.\n",
    "  '''\n",
    "  A = array([[a[0],a[1],a[2]],[b[0],b[1],b[2]],[c[0],c[1],c[2]]])\n",
    "  return my_det(A,3)\n",
    "\n",
    " # Test on the identity matrix:\n",
    "A = identity(3)\n",
    "print(my_det(A,3))\n",
    "print(linalg.det(A))  #compare with built-in function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should write your review here. \n",
    "Things you could choose to discuss:\n",
    "- Code structure \n",
    "- Code style \n",
    "- Does it answer the brief?\n",
    "- Does it work? If not could it be fixed?\n",
    "- Can you explain what it does?\n",
    "\n",
    "Keep your answer relatively brief (approx. 500 words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my_det(), the 'n' should be calculated inside the function, as it is dependent entirely on 'A'. Its calculation would be simple, using the ðš•ðšŽðš—ðšðšðš‘() function on the first element of the input array. It should be put in the docstring for my_det() that A should be square, and there could also be error handling for a non-square input. Additionally, the base case should be for n=2, as it is simply (ad-bc). Changing the base case to this reduces the number of function calls, and the number of elementary operations - improving the efficiency. Still with my_det(), $\\texttt{from numpy import *}$ is advised against in Python. When examining the code in future, it is harder to discern which functions and methods are from numpy. Furthermore, if a user wishes to add to the script, they must be careful not to overwrite any numpy objects with their own. \n",
    "\n",
    "Furthermore, the user has failed to create a docstring in my_det(), as docstrings are created with triple quotes. Even supposing it worked, the â€˜docstringâ€™ is not descriptive. For example it fails to mention what type and dimensions arguments should be. Similarly, the docstring for paraVol() is poor, for the same reason. Following on from the theme of comments, \"#implement the formula\" is pointless, as it describes nothing. Something like \"Calculate the determinant of A recursively using expansion of the first column\" would provide more detail to readers. \"#print(C)\" is poorly styled, and is a debugging technique that should not have been left in.\n",
    "\n",
    "The function names are inconsistent which is poor style. PEP8 recommends lower case words separated by underscores, but the user should focus on keeping the names consistent. \n",
    "\n",
    "\n",
    "The style for paraVol() is poor - the matrix construction should be broken up onto multiple lines.\n",
    "\n",
    "The brief says \"without using the inbuilt functionsâ€, but the code for my_det() uses the numpy function delete(). Additionally, the code for both functions does not work.\n",
    "\n",
    "Consider my_det(), and the following matrix:\n",
    "\n",
    "$$\n",
    "\\left(\\begin{array}{cc} \n",
    "1 & 2\\\\\n",
    "2 & 4\n",
    "\\end{array}\\right)\n",
    "$$ \n",
    "\n",
    "\n",
    "The rows are linearly dependent, meaning a 0 determinant, but applying my_det() to this matrix returns a determinant of -4. The way to fix the function is by adding 1 to the upper end of the loop range, and adding 1 to the exponent of -1 in the updating of 'det'. We need the loop to occur n times, as the formula states. The exponents are a mathematical tool, so their values should relate to indexing starting at 1 (as we do in mathematics when considering matrices), instead of 0.\n",
    "\n",
    "Regarding paraVol(), negative volumes are possible, as determinants can be negative. This can be fixed with the abs() function, assuming my_det() works correctly.It also accepts arguments from higher dimensional space than 3D, which would not be a parallelepiped.  \n",
    "\n",
    "paraVol() combines the the first 3 elements of each argument array into a matrix, and calculates the determinant of that matrix to return the volume.\n",
    "\n",
    "my_det() recurisvely calculates the determinant, with the base case being n=1  (A is mathematically scalar). If A is non-scalar, the code deletes row index 'i' and column 1 (index j), and uses the given formula to get the determinant of the reduced matrix in the same way until the base case. Once the determinant of the reduced matrix has been calculated, the determinant of the current matrix is updated as per the formula. This is repeated for all rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##Â Task 2 - K-Nearest-neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a - First implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task you will need to implement a [K-nearest-neighbours](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) algorithm for classification from scratch (**important - do not use a version of this algorithm from another module e.g SKLearn - you need to write the functions as directed yourself**). The K-nearest-neighbours algorithm is a classic machine learning algorithm used for [classification problems](https://en.wikipedia.org/wiki/Statistical_classification). Classifying a data item from a given dataset means deciding which of a number of classes the item belongs to. This is done using a *training set*, containing a number of data items with known class.\n",
    "\n",
    "We will consider first a simple implementation of the 3-nearest-neighbour version of the algorithm. The algorithm proceeds as follows.\n",
    "\n",
    "For each item in the dataset, we find the 3 nearest items (and their respective classes) in the training set. We then attribute a class to our data item by majority voting amongst the classes of the 3 nearest neighbours. In the case where three classes are tied in the vote, we resolve the tie by choosing the class of whichever neighbour is the nearest in the training set. (In practice this might not be a very good thing to do - but we are just constructing a simple implementation for now). Finally, we need to output the class we have decided on for each item in our original dataset.\n",
    "\n",
    "The data is contained in two files `main_data.txt` and `train_data.txt`.\n",
    "* Each item in `main_data.txt` must be classified into one of 7 classes, using the training set `train_data.txt`.\n",
    "* Each line of `train_data.txt` defines 1 training item, as a vector of floating-point values, followed by its class label (a single integer in the range 1-7).\n",
    "* Each line of `main_data.txt` defines 1 data item, as only a vector of floating-point values, without a class label.\n",
    "\n",
    "To do this write 3 functions:\n",
    "\n",
    "- **`data_setup()`** which reads in both the dataset contained in `main_data.txt`, and the training dataset contained in `train_data.txt` into two lists - returning the two lists (`main_data`, and `train_data`) from the function. The lists containing each item of the dataset should use appropriate types for each element of the list. \n",
    "- **`dist_vect(item,train_data)`** which takes as arguments one element of the list `main_data`, and the list `train_data`, both from the output of the data_setup function. The function should calculate the Euclidean distance from item in the dataset to each item in the training dataset in order. The function should return a list of (distance, class) tuples.\n",
    "- **`decide_class(dv)`** which takes the output list of tuples of the dist_vect function as an argument, and uses it to return the class of the item in the dataset.\n",
    "\n",
    "- Lastly use your functions to obtain a list containing the calculated class for each of the element in the `main_data` list you have constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing 3 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_setup():\n",
    "    '''\n",
    "    data_setup()\n",
    "    \n",
    "    A function that reads in the training and testing data from .txt files which\n",
    "    have the form of a csv. The names of the files are 'train_data.txt' and\n",
    "    'main_data.txt'. These are fixed, and as such the function takes no arguments. \n",
    "    \n",
    "    \n",
    "    To assign, use tuple unpacking. \n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    \n",
    "    (main_data, train_data) : Tuple. \n",
    "        Each element (data) is a list of tuples. Main_data tuples have length n, \n",
    "        and train_data tuples have length n+1, due to including the label (aka class). \n",
    "    \n",
    "    '''\n",
    "    train_data = list()\n",
    "    main_data = list()\n",
    "    \n",
    "\n",
    "    \n",
    "    # As mentioned, each item is an observation, with various features and a class as the final column\n",
    "    #\n",
    "    # For each item in training data, split the item string read in by open() on commas to, \n",
    "    # get a list of numeric strings (e.g. '3.14') and convert each element of the resulting \n",
    "    # list into a float. \n",
    "    #\n",
    "    # Then convert the last element of the item (i.e. the class) into an integer\n",
    "    #\n",
    "    # Convert the list to a tuple for speed purposes in future use, and append this tuple to train_data\n",
    "    \n",
    "    with open('train_data.txt', 'r') as traindata:\n",
    "        for line in traindata:\n",
    "            item_list_train = [float(num) for num in line.split(',')]\n",
    "            item_list_train[-1] = int(item_list_train[-1])\n",
    "            train_data.append(tuple(item_list_train))\n",
    "    traindata.close()\n",
    "    \n",
    "    # As above, without the conversion step, as main_data does not have known classes\n",
    "    \n",
    "    \n",
    "    with open('main_data.txt', 'r') as maindata:\n",
    "        for line in maindata:\n",
    "            item_list_main = [float(num) for num in line.split(',')]\n",
    "            main_data.append(tuple(item_list_main))\n",
    "    maindata.close()\n",
    "    \n",
    "    return (main_data, train_data)\n",
    "\n",
    "def dist_vect(item, train_data):\n",
    "    '''\n",
    "    dist_vect(item, train_data)\n",
    "    \n",
    "    Calculates the L2 distance between the item, and each item (tuple) in train_data\n",
    "    and returns a list of tuples with this distance, and the class of the training item.\n",
    "    \n",
    "    \n",
    "    Arguments\n",
    "    ------------\n",
    "    item: tuple of length n\n",
    "    train_data: List of tuples, each tuple with length n+1 \n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    distclasslist: A list of (distance, class) tuples\n",
    "        The L2 distance between the item, and each tuple in train_data, and \n",
    "        the class corresponding to the tuple in train_data.\n",
    "    \n",
    "    Examples\n",
    "    ---------\n",
    "    \n",
    "    Data for input to function\n",
    "    \n",
    "    >>> item_ex =       (0, 5, 2, 1, 2, 7, 8)\n",
    "    >>> train_data_ex = [(2, 2, 5, 1, 2, 5, 7, 1), (6, 2, 5, 1, 1, 3, 5, 2), (3, 3, 1, 1, 7, 3, 2, 2)]\n",
    "    \n",
    "    Output          \n",
    "    \n",
    "    >>> print(dist_vect(item = item_ex, train_data = train_data_ex))\n",
    "    [(5.196152422706632, 1), (8.94427190999916, 2), (9.539392014169456, 2)]\n",
    "    \n",
    "    (5.1962, 1) means the L2 distance between item_ex and the first item of\n",
    "    the training data is 5.1962, and the class of the first item in the\n",
    "    training data is 1. \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Will store (distance, class) tuples here\n",
    "    dist_class_list = list()\n",
    "    \n",
    "    # Only want to iterate for each feature in the item (which is unclassified)  \n",
    "    main_length = len(item)\n",
    "\n",
    " \n",
    "    # Create list of squared differences, sum this list, and square root the sum, to\n",
    "    # calculate Euclidean distance.\n",
    "    \n",
    "    # Then extract the class of the training item, which is the final element in train_item\n",
    "    \n",
    "    for train_item in train_data:\n",
    "        dist = sum([ (item[i] - train_item[i]) ** 2 for i in range(main_length) ]) ** 0.5\n",
    "        train_class = train_item[-1]\n",
    "        dist_class_list.append((dist, train_class))\n",
    "        \n",
    "    return dist_class_list\n",
    "\n",
    "\n",
    "def decide_class(dv):\n",
    "    '''\n",
    "    decide_class(dv)\n",
    "    \n",
    "    Makes a prediction for the class of the item that created dv, by using a nearest-neighbours\n",
    "    decision methodology, with k = 3. \n",
    "    \n",
    "    \n",
    "    Arguments\n",
    "    -------------\n",
    "    dv: A list of (distance, class) tuples\n",
    "        Of the form outputted by the dist_vect functions. \n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    nn_class: Integer\n",
    "        The predicted class of the item that produced the distance vector dv. This\n",
    "        predicted class is calculated using k-nearest-neighbours classification\n",
    "        where k = 3, and using simple majority voting. \n",
    "        \n",
    "    \n",
    "    Examples\n",
    "    -------------\n",
    "    \n",
    "    Using distance vector output example from dist_vect() docstring\n",
    "    \n",
    "    >>> dv_ex = [(5.196152422706632, 1), (8.94427190999916, 2), (9.539392014169456, 2)]\n",
    "    >>> print(decide_class(dv_ex))\n",
    "    2\n",
    "    \n",
    "    Modifying slightly so there is no modal class\n",
    "    \n",
    "    >>> dv_ex_2  =  [(5.196152422706632, 1), (8.94427190999916, 2), (9.539392014169456, 3)]\n",
    "    >>> print(decide_class(dv_ex_2))\n",
    "    1\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    # Find 3 nearest. Sorted automatically sorts by first element\n",
    "    \n",
    "    dv_nearest_k = sorted(dv)[0:3]\n",
    "    \n",
    "    # Count classes generally\n",
    "    # find highest integer class in dv\n",
    "    # Create countlist, where indices correspond to class \n",
    "    # i.e. index 1 contains counts of class 1 etc. up to the highest class\n",
    "    # count_list will automatically expand if more classes are added\n",
    "    \n",
    "    highest_class = max(dv, key = lambda x: x[1])[1]\n",
    "    count_list = [0]*(highest_class+1)\n",
    "    \n",
    "    # Extract class from tuple, and add 1 to corresponding index in countlist to keep track of count\n",
    "    # Recalling that class corresponds to list index. \n",
    "    for dv_tuple in dv_nearest_k:\n",
    "        dv_class = dv_tuple[1]\n",
    "        count_list[dv_class] += 1\n",
    "    \n",
    "    # classandcount matches each index (class) with its count, tuple of (class, count) tuples\n",
    "    \n",
    "    class_and_count = tuple(enumerate(count_list))\n",
    "    \n",
    "    # If one class greatest, return it\n",
    "    # First, find maximum count in countlist\n",
    "\n",
    "    max_count = max(count_list)\n",
    "    \n",
    "    # Create list of the indices (classes) where their count is the max count\n",
    "\n",
    "    max_list = [index for index, count in class_and_count if count == max_count]\n",
    "    \n",
    "    # DECISION\n",
    "    \n",
    "    # If max_list has length 1, there is a unique maximum, hence return that class\n",
    "    # Otherwise find the nearest neighbour (minimum distance class)\n",
    "    \n",
    "    \n",
    "    if len(max_list) == 1:\n",
    "        nn_class = max_list[0]\n",
    "    else:\n",
    "        # dv_nearest_k already sorted\n",
    "        # First element is a (distance, class) tuple, use [1] to access class\n",
    "        nn_class = dv_nearest_k[0][1]\n",
    "    \n",
    "    return nn_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lastly use your functions to obtain a list containing the calculated class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to predict the classes. \n",
    "\n",
    "def predict_knn_classes(datasets, distancecalc, classdecision):\n",
    "    '''\n",
    "    predict_knn_classes(datasets, distancecalc, classdecision)\n",
    "        Function that outputs the predicted classes of the main_data, given a distance calculation and\n",
    "        a particular decision methodology with the number of nearest neighbours considered set to 3 by\n",
    "        default.\n",
    "\n",
    "    Arguments\n",
    "    -------------\n",
    "    datasets: Function\n",
    "        Outputs a (main_data, train_data) tuple with each element being a list of tuples\n",
    "    \n",
    "    distancecalc: Function\n",
    "        Calculates the distance from a tuple from the main_data list (tuples have length n) to each tuple \n",
    "        in the train_data (tuples have length n+1).\n",
    "        \n",
    "        Outputs a list of (distance, class) tuples, where the class is from the last element of the tuple \n",
    "        in the train_data.\n",
    "    \n",
    "    classdecision: Function\n",
    "        Uses the output from distancecalc to classify the item passed into distanecalc, given a k-nearest-neighbours \n",
    "        method with majority voting, using k=3. \n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    main_class_predictions: List of integers\n",
    "        A list of class predictions for each item in main_data. \n",
    "\n",
    "    '''\n",
    "    # Read and assign data using function passed through to datasets\n",
    "    # Initialise a list of predictions\n",
    "    \n",
    "    main_data, train_data = datasets()\n",
    "    main_class_predictions = list()\n",
    "    \n",
    "    # For each unclassified item, create the list of (distance, class) tuples using the \n",
    "    # function passed to distancecalc\n",
    "    \n",
    "    # Using this list, predict the most appropriate class using the decision methodology\n",
    "    # provided in the function passed to classdecision, and append it to the list of predictions\n",
    "        \n",
    "    for item in main_data:\n",
    "        dv = distancecalc(item, train_data = train_data)\n",
    "        main_class_predictions.append(classdecision(dv = dv))\n",
    "    \n",
    "    \n",
    "    return(main_class_predictions)\n",
    "\n",
    "# Use the 3 functions requested in the task as inputs to my function to\n",
    "# predict the class of each item in the main_data.txt. \n",
    "\n",
    "print('L2 norm predictions: \\n')\n",
    "print(predict_knn_classes(data_setup, dist_vect, decide_class)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Task 2b - Changing the distance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next write two different replacements for the **dist_vect(item,train_data)** function. The function you have written above calculates the distance between data items using Euclidean distance as:\n",
    "$$d(x,y) = \\left(\\sum_{i=1}^n (x_i-y_i)^2\\right)^{1/2}$$\n",
    "\n",
    "For this task write two more versions of the distance function using different distance metrics.\n",
    "\n",
    "- Firstly **dist_vectL1(item,train_data)** where distance is determined by using:\n",
    "\n",
    "$$d(x,y) = \\sum_{i=1}^n \\left|x_i-y_i\\right|$$\n",
    "\n",
    "- Next, **dist_vectLinf(item,train_data)** where distance is determined by using:\n",
    "\n",
    "$$d(x,y) = \\max\\limits_{i=1..n}\\left|x_i-y_i\\right|$$\n",
    "\n",
    "These are the $L_1$ and $L_\\infty$ norms respectively. \n",
    "\n",
    "- Lastly use the new functions to obtain a list containing the calculated class for each of the element in the main_data list you have constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_vectL1(item, train_data):\n",
    "    '''\n",
    "    dist_vectL1(item, train_data)\n",
    "        Calculates the L1 distance between the item, and each item (tuple) in train_data and returns a list of \n",
    "        tuples with this distance, and the class of the training item.\n",
    "    \n",
    "    Arguments\n",
    "    ------------\n",
    "    item: tuple of length n\n",
    "    train_data: List of tuples, each tuple with length n+1 \n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    distclasslist: A list of (distance, class) tuples\n",
    "        The L1 distance between the item, and each tuple in train_data, and \n",
    "        the class corresponding to the tuple in train_data.\n",
    "    \n",
    "    Examples\n",
    "    -------------\n",
    "    \n",
    "    Data for input to function\n",
    "    \n",
    "    >>> item_ex =       (0, 5, 2, 1, 2, 7, 8)\n",
    "    >>> train_data_ex = [(2, 2, 5, 1, 2, 5, 7, 1), (6, 2, 5, 1, 1, 3, 5, 2), (3, 3, 1, 1, 7, 3, 2, 2)]\n",
    "    \n",
    "    Output          \n",
    "    \n",
    "    >>> print(dist_vectL1(item = item_ex, train_data = train_data_ex))\n",
    "    [(11, 1), (20, 2), (21, 2)]\n",
    "    \n",
    "    (11, 1) means the L1 distance between item_ex and the first item of\n",
    "    the training data is 11, and the class of the first item in the\n",
    "    training data is 1.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Initialise list of (distance, class) tuples\n",
    "    dist_class_list = list()\n",
    "    \n",
    "    # Only want to iterate for each feature in the unclassified item\n",
    "    main_length = len(item)\n",
    "\n",
    "    # Create a list of the absolute differences between each feature of the item and the training item\n",
    "    # and then sum this, and append it to dist_class_list, along with the class of the training item. \n",
    "    \n",
    "    for train_item in train_data:\n",
    "        dist = sum([ abs((item[i] - train_item[i])) for i in range(main_length) ])\n",
    "        train_class = train_item[-1]\n",
    "        dist_class_list.append((dist, train_class))\n",
    "        \n",
    "    return dist_class_list\n",
    "\n",
    "def dist_vectLinf(item, train_data):\n",
    "    '''\n",
    "    dist_vectLinf(item, train_data) \n",
    "        Calculates the L-infinity distance between the item, and each item (tuple) in train_data and returns \n",
    "        a list of tuples with this distance, and the class of the training item. \n",
    "    \n",
    "    Arguments\n",
    "    ------------\n",
    "    item: tuple of length n\n",
    "    train_data: List of tuples, each tuple with length n+1 \n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    distclasslist: A list of (distance, class) tuples\n",
    "        The L-infinity distance between the item, and each tuple in train_data, and \n",
    "        the class corresponding to the tuple in train_data.\n",
    "    \n",
    "    Examples\n",
    "    ---------\n",
    "    \n",
    "    Data for input to function\n",
    "    \n",
    "    >>> item_ex =       (0, 5, 2, 1, 2, 7, 8)\n",
    "    >>> train_data_ex = [(2, 2, 5, 1, 2, 5, 7, 1), (6, 2, 5, 1, 1, 3, 5, 2), (3, 3, 1, 1, 7, 3, 2, 2)]\n",
    "    \n",
    "    Output          \n",
    "    \n",
    "    >>> print(dist_vectLinf(item = item_ex, train_data = train_data_ex))\n",
    "    [(3, 1), (6, 2), (6, 2)]\n",
    "    \n",
    "    (3, 1) means the LInf distance between item_ex and the first item of\n",
    "    the training data is 5.1962, and the class of the first item in the\n",
    "    training data is 1.\n",
    "    \n",
    "    '''\n",
    "    # Initialise list of (distance, class) tuples\n",
    "    dist_class_list = list()\n",
    "    \n",
    "    # Only want to iterate for each feature in the unclassified item\n",
    "    main_length = len(item)\n",
    "\n",
    "    # Create a list of the absolute differences between each feature of the item and the training item\n",
    "    # and then find the maximum difference, and append it to dist_class_list, along with the class of \n",
    "    # the training item. \n",
    "    \n",
    "    for train_item in train_data:\n",
    "        dist = max([ abs((item[i] - train_item[i])) for i in range(main_length) ])\n",
    "        train_class = train_item[-1]\n",
    "        dist_class_list.append((dist, train_class))\n",
    "    \n",
    "    return dist_class_list\n",
    "\n",
    "\n",
    "\n",
    "print('L1 Norm predictions \\n')\n",
    "print(predict_knn_classes(datasets=data_setup, distancecalc=dist_vectL1,classdecision=decide_class), '\\n')\n",
    "print('L infinity Norm predictions \\n')\n",
    "print(predict_knn_classes(datasets=data_setup, distancecalc=dist_vectLinf,classdecision=decide_class))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2c - Changing the decision methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple majority-voting implemented in your `decide_class()` function is unweighted --- once the 3 nearest neighbours are found, they each have 1 vote, regardless of which is closest (unless there is a tie). An alternative is *weighted voting*, where each of the $k$ nearest neighbours is attributed a coefficient, such that the closest neighbours carry more weight in the result.\n",
    "\n",
    "Write a function `decide_class_wk(dv,k)` which also takes the number $k$ of nearest neighbours as an input argument. Your function should attribute a weight $w_j$ to each of the $k$ nearest neighbours, inversely proportional to their distance from the data item:\n",
    "\n",
    "$$\n",
    "w_j = \\frac{1}{d_j(x,y)}\\, , \\quad j \\in \\{1,\\ldots,k\\}\n",
    "$$\n",
    "\n",
    "The score for each class is determined by the sum of the weights of each item in that class amongst the $k$ nearest neighbours. The class with the highest score is chosen for the data item.\n",
    "\n",
    "(Note that the simple majority-voting corresponds to setting all the weights to 1.)\n",
    "\n",
    "Test your function on the dataset, for $k=3, 8,$ and $25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define decide_class_wk(dv, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_class_wk(dv, k):\n",
    "    '''\n",
    "    decide_class_wk(dv, k)\n",
    "        Calculates the most appropriate class for an item which produced a list of (distance, class) tuples \n",
    "        against a training set, using k-nearest-neighbours classification.\n",
    "    \n",
    "    Arguments\n",
    "    -------------\n",
    "    dv: A list of (distance, class) tuples\n",
    "        Of the form outputted by the dist_vect functions. \n",
    "    k:  Integer\n",
    "        Number of nearest neighbours to consider before applying the decision\n",
    "        methodology\n",
    "    \n",
    "    Returns\n",
    "    -------------\n",
    "    nn_class: Integer\n",
    "        The predicted class of the item that produced the distance vector dv. This\n",
    "        predicted class is calculated using k-nearest-neighbours classification\n",
    "        with weighted scoring.\n",
    "    \n",
    "    Examples\n",
    "    -------------\n",
    "    \n",
    "    >>> dv_ex = [(5.196152422706632, 1), (8.94427190999916, 2), (9.539392014169456, 2)]\n",
    "    >>> print(decide_class_wk(dv_ex, 3))\n",
    "    2\n",
    "    \n",
    "    Modifying so effect of weights can be seen.\n",
    "    \n",
    "    With weight 1, class 2 would win by majority. But with weighted voting, the fact class 1 is very \n",
    "    close to the item outweights the fact class 2 has more points near the item. \n",
    "    \n",
    "    >>> dv_ex_2  =  [(1, 1), (8, 2), (9, 2)]\n",
    "    >>> print(decide_class_wk(dv_ex_2, 3))\n",
    "    1\n",
    "\n",
    "    '''\n",
    "    # Get nearest k\n",
    "    dv_nearest_k = sorted(dv)[0:k]\n",
    "    \n",
    "    # Find highest class in dv\n",
    "    # Initialise score_list, which will store score, where index of score_list \n",
    "    # corresponds to class. \n",
    "    # score_list will automatically expand if more classes are added\n",
    "    \n",
    "    highest_class = max(dv, key = lambda x: x[1])[1]\n",
    "    score_list = [0]*(highest_class+1)\n",
    "    \n",
    "    # Calculate weight and extract class\n",
    "    # Update the total weight corresponding to the class in scorelist.\n",
    "    \n",
    "    for dv_tuple in dv_nearest_k:\n",
    "        weight = (dv_tuple[0]) ** (-1)  \n",
    "        dv_class = dv_tuple[1]\n",
    "        score_list[dv_class] += weight\n",
    "    \n",
    "    # Find index (hence class) corresponding to highest score, and return it.\n",
    "    \n",
    "    nn_class = score_list.index(max(score_list))\n",
    "    return nn_class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function for k $\\in$ {3, 8, 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to predict the classes. \n",
    "\n",
    "def predict_knn_classes(datasets, distancecalc, classdecision, k):\n",
    "    '''\n",
    "    predict_knn_classes(datasets, distancecalc, classdecision, k)\n",
    "        Function that outputs the predicted classes of the main_data, given a distance calculation and\n",
    "        a particular decision methodology with the number of nearest neighbours considered. \n",
    "\n",
    "    Arguments\n",
    "    -------------\n",
    "    datasets: Function\n",
    "        Outputs a (main_data, train_data) tuple with each element being a list \n",
    "        of tuples\n",
    "    k: Integer\n",
    "        How many nearest neighbours to consider when applying decision methodology\n",
    "    \n",
    "    distancecalc: Function\n",
    "        Calculates the distance from a tuple from the main_data list (tuples \n",
    "        have length n) to each tuple in the train_data (tuples have length n+1).\n",
    "        Outputs a list of (distance, class) tuples, where the class is from the\n",
    "        last element of the tuple in the train_data.\n",
    "    \n",
    "    classdecision: Function\n",
    "        Uses the output from distancecalc to classify the item passed into\n",
    "        distanecalc, given a k-nearest-neighbours method with majority voting,\n",
    "        using k=3. \n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    main_class_predictions: List of integers\n",
    "        A list of class predictions for each item in main_data. \n",
    "\n",
    "    '''\n",
    "    # Read and assign data using function passed through to datasets\n",
    "    # Initialise a list of predictions\n",
    "\n",
    "    # Make a prediction for each item in the main_data using the distance calculation\n",
    "    # given, and the class decision method. \n",
    "    \n",
    "    main_data, train_data = datasets()\n",
    "    main_class_predictions = list()\n",
    "    \n",
    "    # For each unclassified item, create the list of (distance, class) tuples using the \n",
    "    # function passed to distancecalc\n",
    "    \n",
    "    # Using this list, predict the most appropriate class using the decision methodology\n",
    "    # provided in the function passed to classdecision, and append it to the list of predictions\n",
    "        \n",
    "    for item in main_data:\n",
    "        dv = distancecalc(item, train_data = train_data)\n",
    "        main_class_predictions.append(classdecision(dv = dv, k = k))\n",
    "    \n",
    "    \n",
    "    return(main_class_predictions)\n",
    "\n",
    "# Use the 3 functions requested in the task as inputs to my function to\n",
    "# predict the class of each item in the main_data.txt. \n",
    "\n",
    "for i in (3, 8, 25):\n",
    "    print('For k =', i, 'using L2 distance, the predictions for the main_data are: \\n')\n",
    "    print(predict_knn_classes(data_setup, dist_vect, decide_class_wk, k = i), '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
